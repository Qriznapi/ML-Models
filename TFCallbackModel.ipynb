{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b238cf8-ef7f-4eba-b71a-7cfb95f0fa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda1\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After batch 0, the loss is 29446.60.\n",
      "\n",
      " After batch 2, the loss is 29600.22.\n",
      "Epoch 0: Average loss is 29267.85, mean absolute error is  152.50.\n",
      "\n",
      " After batch 0, the loss is 32516.29.\n",
      "\n",
      " After batch 2, the loss is 30130.11.\n",
      "Epoch 1: Average loss is 29190.79, mean absolute error is  152.25.\n",
      "\n",
      " After batch 0, the loss is 23798.37.\n",
      "\n",
      " After batch 2, the loss is 29622.43.\n",
      "Epoch 2: Average loss is 29093.98, mean absolute error is  151.93.\n",
      "\n",
      " After batch 0, the loss is 27281.22.\n",
      "\n",
      " After batch 2, the loss is 27499.39.\n",
      "Epoch 3: Average loss is 28948.98, mean absolute error is  151.45.\n",
      "\n",
      " After batch 0, the loss is 29771.51.\n",
      "\n",
      " After batch 2, the loss is 29101.61.\n",
      "Epoch 4: Average loss is 28738.28, mean absolute error is  150.77.\n",
      "\n",
      " After batch 0, the loss is 27811.62.\n",
      "\n",
      " After batch 2, the loss is 27732.78.\n",
      "Epoch 5: Average loss is 28421.46, mean absolute error is  149.79.\n",
      "\n",
      " After batch 0, the loss is 23985.71.\n",
      "\n",
      " After batch 2, the loss is 27462.57.\n",
      "Epoch 6: Average loss is 27983.82, mean absolute error is  148.43.\n",
      "\n",
      " After batch 0, the loss is 30622.71.\n",
      "\n",
      " After batch 2, the loss is 26208.05.\n",
      "Epoch 7: Average loss is 27407.77, mean absolute error is  146.60.\n",
      "\n",
      " After batch 0, the loss is 30801.31.\n",
      "\n",
      " After batch 2, the loss is 26995.64.\n",
      "Epoch 8: Average loss is 26642.58, mean absolute error is  144.15.\n",
      "\n",
      " After batch 0, the loss is 27140.48.\n",
      "\n",
      " After batch 2, the loss is 25789.72.\n",
      "Epoch 9: Average loss is 25636.95, mean absolute error is  140.97.\n",
      "\n",
      " After batch 0, the loss is 25431.42.\n",
      "\n",
      " After batch 2, the loss is 23806.02.\n",
      "Epoch 10: Average loss is 24419.06, mean absolute error is  136.96.\n",
      "\n",
      " After batch 0, the loss is 23932.49.\n",
      "\n",
      " After batch 2, the loss is 23065.99.\n",
      "Epoch 11: Average loss is 22979.90, mean absolute error is  132.06.\n",
      "\n",
      " After batch 0, the loss is 21140.73.\n",
      "\n",
      " After batch 2, the loss is 21882.10.\n",
      "Epoch 12: Average loss is 21299.59, mean absolute error is  125.99.\n",
      "\n",
      " After batch 0, the loss is 19980.02.\n",
      "\n",
      " After batch 2, the loss is 19610.43.\n",
      "Epoch 13: Average loss is 19394.21, mean absolute error is  118.84.\n",
      "\n",
      " After batch 0, the loss is 18363.62.\n",
      "\n",
      " After batch 2, the loss is 17270.24.\n",
      "Epoch 14: Average loss is 17194.50, mean absolute error is  110.18.\n",
      "\n",
      " After batch 0, the loss is 15379.74.\n",
      "\n",
      " After batch 2, the loss is 15838.08.\n",
      "Epoch 15: Average loss is 14941.59, mean absolute error is  100.69.\n",
      "\n",
      " After batch 0, the loss is 11897.22.\n",
      "\n",
      " After batch 2, the loss is 12443.92.\n",
      "Epoch 16: Average loss is 12707.28, mean absolute error is   90.79.\n",
      "\n",
      " After batch 0, the loss is 10169.07.\n",
      "\n",
      " After batch 2, the loss is 10936.11.\n",
      "Epoch 17: Average loss is 10672.00, mean absolute error is   82.03.\n",
      "\n",
      " After batch 0, the loss is 5574.26.\n",
      "\n",
      " After batch 2, the loss is 8942.76.\n",
      "Epoch 18: Average loss is 8761.27, mean absolute error is   73.00.\n",
      "\n",
      " After batch 0, the loss is 8117.85.\n",
      "\n",
      " After batch 2, the loss is 7443.23.\n",
      "Epoch 19: Average loss is 7562.80, mean absolute error is   67.49.\n",
      "\n",
      " After batch 0, the loss is 12541.42.\n",
      "\n",
      " After batch 1, the loss is 15832.36.\n",
      "\n",
      " After batch 2, the loss is 18797.15.\n",
      "\n",
      " After batch 3, the loss is 19400.75.\n",
      "\n",
      " After batch 4, the loss is 19483.79.\n",
      "Finished prediction on batch 0!\n",
      "Finished prediction on batch 1!\n",
      "Finished prediction on batch 2!\n",
      "Finished prediction on batch 3!\n",
      "Finished prediction on batch 4!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error: Optimizer does not have a learning rate.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 123\u001b[0m\n\u001b[0;32m    117\u001b[0m new_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    118\u001b[0m                 optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    119\u001b[0m                 metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Fit the model with our learning rate scheduler callback\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m new_history \u001b[38;5;241m=\u001b[39m new_model\u001b[38;5;241m.\u001b[39mfit(train_data, train_targets, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m    124\u001b[0m                             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[LRScheduler(get_new_epoch_lr)], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Anaconda1\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[6], line 92\u001b[0m, in \u001b[0;36mLRScheduler.on_epoch_begin\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# Make sure that the optimizer we have chosen has a learning rate, and raise an error if not\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m           \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError: Optimizer does not have a learning rate.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# Get the current learning rate\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     curr_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mget_value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mlr))\n",
      "\u001b[1;31mValueError\u001b[0m: Error: Optimizer does not have a learning rate."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# Load the diabetes dataset\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes_dataset = load_diabetes()\n",
    "\n",
    "# Save the input and target variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = diabetes_dataset['data']\n",
    "targets = diabetes_dataset['target']\n",
    "\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)        \n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer=\"adam\", metrics=['mae'])\n",
    "\n",
    "# Create the custom callback\n",
    "\n",
    "class LossAndMetricCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    # Print the loss after every second batch in the training set\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch %2 ==0:\n",
    "            print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n",
    "    \n",
    "    # Print the loss after each batch in the test set\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n",
    "\n",
    "    # Print the loss and mean absolute error after each epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Epoch {}: Average loss is {:7.2f}, mean absolute error is {:7.2f}.'.format(epoch, logs['loss'], logs['mae']))\n",
    "    \n",
    "    # Notify the user when prediction has finished on each batch\n",
    "    def on_predict_batch_end(self,batch, logs=None):\n",
    "        print(\"Finished prediction on batch {}!\".format(batch))\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=20,\n",
    "                    batch_size=100, callbacks=[LossAndMetricCallback()], verbose=False)\n",
    "\n",
    "model_eval = model.evaluate(test_data, test_targets, batch_size=10, \n",
    "                            callbacks=[LossAndMetricCallback()], verbose=False)\n",
    "\n",
    "model_pred = model.predict(test_data, batch_size=10,\n",
    "                           callbacks=[LossAndMetricCallback()], verbose=False)\n",
    "\n",
    "# Define the learning rate schedule. The tuples below are (start_epoch, new_learning_rate)\n",
    "\n",
    "lr_schedule = [\n",
    "    (4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)\n",
    "]\n",
    "\n",
    "def get_new_epoch_lr(epoch, lr):\n",
    "    # Checks to see if the input epoch is listed in the learning rate schedule \n",
    "    # and if so, returns index in lr_schedule\n",
    "    epoch_in_sched = [i for i in range(len(lr_schedule)) if lr_schedule[i][0]==int(epoch)]\n",
    "    if len(epoch_in_sched)>0:\n",
    "        # If it is, return the learning rate corresponding to the epoch\n",
    "        return lr_schedule[epoch_in_sched[0]][1]\n",
    "    else:\n",
    "        # Otherwise, return the existing learning rate\n",
    "        return lr\n",
    "\n",
    "# Define the custom callback\n",
    "\n",
    "class LRScheduler(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, new_lr):\n",
    "        super(LRScheduler, self).__init__()\n",
    "        # Add the new learning rate function to our callback\n",
    "        self.new_lr = new_lr\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Make sure that the optimizer we have chosen has a learning rate, and raise an error if not\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "              raise ValueError('Error: Optimizer does not have a learning rate.')\n",
    "                \n",
    "        # Get the current learning rate\n",
    "        curr_rate = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "        \n",
    "        # Call the auxillary function to get the scheduled learning rate for the current epoch\n",
    "        scheduled_rate = self.new_lr(epoch, curr_rate)\n",
    "\n",
    "        # Set the learning rate to the scheduled learning rate\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n",
    "        print('Learning rate for epoch {} is {:7.3f}'.format(epoch, scheduled_rate))\n",
    "\n",
    "# Build the same model as before\n",
    "\n",
    "new_model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)        \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "new_model.compile(loss='mse',\n",
    "                optimizer=\"adam\",\n",
    "                metrics=['mae', 'mse'])\n",
    "\n",
    "# Fit the model with our learning rate scheduler callback\n",
    "\n",
    "new_history = new_model.fit(train_data, train_targets, epochs=20,\n",
    "                            batch_size=100, callbacks=[LRScheduler(get_new_epoch_lr)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa600622-7c88-42d3-a491-aa19e5d116b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
