{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949100df-6b9a-477e-bb6f-431344d60024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "from numpy.random import seed\n",
    "seed(8)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection \n",
    "%matplotlib inline\n",
    "\n",
    "# If you would like to make further imports from tensorflow, add them here\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.initializers import he_uniform, Constant \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def read_in_and_split_data(iris_data):\n",
    "    \"\"\"\n",
    "    This function takes the Iris dataset as loaded by sklearn.datasets.load_iris(), and then \n",
    "    splits so that the training set includes 90% of the full dataset, with the test set \n",
    "    making up the remaining 10%.\n",
    "    Your function should return a tuple (train_data, test_data, train_targets, test_targets) \n",
    "    of appropriately split training and test data and targets.\n",
    "    \n",
    "    If you would like to import any further packages to aid you in this task, please do so in the \n",
    "    Package Imports cell above.\n",
    "    \"\"\"\n",
    "    data = iris_data['data']\n",
    "    targets = iris_data['target']\n",
    "    train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size = 0.1)\n",
    "    return (train_data, test_data, train_targets, test_targets)\n",
    "\n",
    "# Run your function to generate the test and training data.\n",
    "\n",
    "iris_data = datasets.load_iris()\n",
    "train_data, test_data, train_targets, test_targets = read_in_and_split_data(iris_data)\n",
    "\n",
    "# Convert targets to a one-hot encoding\n",
    "\n",
    "train_targets = tf.keras.utils.to_categorical(np.array(train_targets))\n",
    "test_targets = tf.keras.utils.to_categorical(np.array(test_targets))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc51a2f-18ab-4538-b7bc-813862151222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda1\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "\n",
    "def get_model(input_shape):\n",
    "    \"\"\"\n",
    "    This function should build a Sequential model according to the above specification. Ensure the \n",
    "    weights are initialised by providing the input_shape argument in the first layer, given by the\n",
    "    function argument.\n",
    "    Your function should return the model.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.Constant(value=1), activation=\"relu\", input_shape=input_shape),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(3, activation=\"softmax\")\n",
    "    ])\n",
    "    return model\n",
    "    \n",
    "# Run your function to get the model\n",
    "\n",
    "model = get_model(train_data[0].shape)\n",
    "\n",
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def compile_model(model):\n",
    "    \"\"\"\n",
    "    This function takes in the model returned from your get_model function, and compiles it with an optimiser,\n",
    "    loss function and metric.\n",
    "    Compile the model using the Adam optimiser (with learning rate set to 0.0001), \n",
    "    the categorical crossentropy loss function and accuracy as the only metric. \n",
    "    Your function doesn't need to return anything; the model will be compiled in-place.\n",
    "    \"\"\"\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run your function to compile the model\n",
    "\n",
    "compile_model(model)\n",
    "\n",
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def train_model(model, train_data, train_targets, epochs):\n",
    "    \"\"\"\n",
    "    This function should train the model for the given number of epochs on the \n",
    "    train_data and train_targets. \n",
    "    Your function should return the training history, as returned by model.fit.\n",
    "    \"\"\"\n",
    "    history = model.fit(train_data, train_targets, epochs = epochs,\n",
    "                        validation_split=0.15, batch_size = 40, verbose = False)\n",
    "    return history\n",
    "\n",
    "# Run your function to train the model\n",
    "\n",
    "history = train_model(model, train_data, train_targets, epochs=800)\n",
    "# Run this cell to plot the epoch vs accuracy graph\n",
    "\n",
    "try:\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "except KeyError:\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "plt.title('Accuracy vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "plt.show() \n",
    "\n",
    "#Run this cell to plot the epoch vs loss graph\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e141db71-16a0-4f84-b935-5a70c3a9f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_regularised_model(input_shape, dropout_rate, weight_decay):\n",
    "    \"\"\"\n",
    "    This function should build a regularised Sequential model according to the above specification. \n",
    "    The dropout_rate argument in the function should be used to set the Dropout rate for all Dropout layers.\n",
    "    L2 kernel regularisation (weight decay) should be added using the weight_decay argument to \n",
    "    set the weight decay coefficient in all Dense layers that use L2 regularisation.\n",
    "    Ensure the weights are initialised by providing the input_shape argument in the first layer, given by the\n",
    "    function argument input_shape.\n",
    "    Your function should return the model.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, kernel_initializer=he_uniform(), bias_initializer=Constant(value=1), activation=\"relu\", input_shape=input_shape, kernel_regularizer=l2(weight_decay)),\n",
    "        layers.Dense(128, activation=\"relu\", kernel_regularizer=l2(weight_decay)),\n",
    "        layers.Dense(128, activation=\"relu\", kernel_regularizer=l2(weight_decay)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(128, activation=\"relu\", kernel_regularizer=l2(weight_decay)),\n",
    "        layers.Dense(128, activation=\"relu\", kernel_regularizer=l2(weight_decay)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(weight_decay)),\n",
    "        layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(weight_decay)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(weight_decay)),\n",
    "        layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(weight_decay)),\n",
    "        layers.Dense(3, activation=\"softmax\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Instantiate the model, using a dropout rate of 0.3 and weight decay coefficient of 0.001\n",
    "\n",
    "reg_model = get_regularised_model(train_data[0].shape, 0.3, 0.001)\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "compile_model(reg_model)\n",
    "# Train the model\n",
    "\n",
    "reg_history = train_model(reg_model, train_data, train_targets, epochs=800)\n",
    "#Run this cell to plot the new accuracy vs epoch graph\n",
    "\n",
    "try:\n",
    "    plt.plot(reg_history.history['accuracy'])\n",
    "    plt.plot(reg_history.history['val_accuracy'])\n",
    "except KeyError:\n",
    "    plt.plot(reg_history.history['acc'])\n",
    "    plt.plot(reg_history.history['val_acc'])\n",
    "plt.title('Accuracy vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "plt.show() \n",
    "#Run this cell to plot the new loss vs epoch graph\n",
    "\n",
    "plt.plot(reg_history.history['loss'])\n",
    "plt.plot(reg_history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show() \n",
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_callbacks():\n",
    "    \"\"\"\n",
    "    This function should create and return a tuple (early_stopping, learning_rate_reduction) callbacks.\n",
    "    The callbacks should be instantiated according to the above requirements.\n",
    "    \"\"\"\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=30)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20)\n",
    "    return early_stopping, learning_rate_reduction\n",
    "\n",
    "call_model = get_regularised_model(train_data[0].shape, 0.3, 0.0001)\n",
    "compile_model(call_model)\n",
    "early_stopping, learning_rate_reduction = get_callbacks()\n",
    "call_history = call_model.fit(train_data, train_targets, epochs=800, validation_split=0.15,\n",
    "                         callbacks=[early_stopping, learning_rate_reduction], verbose=0)\n",
    "print(learning_rate_reduction.patience)\n",
    "try:\n",
    "    plt.plot(call_history.history['accuracy'])\n",
    "    plt.plot(call_history.history['val_accuracy'])\n",
    "except KeyError:\n",
    "    plt.plot(call_history.history['acc'])\n",
    "    plt.plot(call_history.history['val_acc'])\n",
    "plt.title('Accuracy vs. epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "plt.show() \n",
    "plt.plot(call_history.history['loss'])\n",
    "plt.plot(call_history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show() \n",
    "# Evaluate the model on the test set\n",
    "\n",
    "test_loss, test_acc = call_model.evaluate(test_data, test_targets, verbose=0)\n",
    "print(\"Test loss: {:.3f}\\nTest accuracy: {:.2f}%\".format(test_loss, 100 * test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
